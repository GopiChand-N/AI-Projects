{
 "cells": [
  {
   "cell_type": "code",
   "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
   "metadata": {
    "height": 64,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T07:57:44.884122Z",
     "start_time": "2025-09-11T07:57:44.837347Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T07:57:47.611440Z",
     "start_time": "2025-09-11T07:57:47.604778Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T07:57:50.921927Z",
     "start_time": "2025-09-11T07:57:50.020206Z"
    }
   },
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"rag.pdf\"]).load_data()\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T07:57:52.927164Z",
     "start_time": "2025-09-11T07:57:52.827335Z"
    }
   },
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:18:05.856668Z",
     "start_time": "2025-09-11T08:18:05.849017Z"
    }
   },
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4.1-mini\", api_key=OPENAI_API_KEY)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "## Define Summary Index and Vector Index over the Same Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T07:58:01.282790Z",
     "start_time": "2025-09-11T07:57:58.109059Z"
    }
   },
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:28:00,142 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "## Define Query Engines and Set Metadata"
   ]
  },
  {
   "cell_type": "code",
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:00:42.603010Z",
     "start_time": "2025-09-11T08:00:42.595134Z"
    }
   },
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 285,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:29:54.323875Z",
     "start_time": "2025-09-11T08:29:54.319562Z"
    }
   },
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "## Define Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 217,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:32:23.101895Z",
     "start_time": "2025-09-11T08:32:23.097455Z"
    }
   },
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:17:43.132865Z",
     "start_time": "2025-09-11T08:17:08.733294Z"
    }
   },
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:47:14,879 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-11 13:47:14,885 - INFO - Selecting query engine 1: The question requests a summary, which is a summarization task; choice (2) is explicitly for summarization questions related to MetaGPT, while choice (1) is for retrieving specific context..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mSelecting query engine 1: The question requests a summary, which is a summarization task; choice (2) is explicitly for summarization questions related to MetaGPT, while choice (1) is for retrieving specific context..\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:47:15,851 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-09-11 13:47:43,123 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The work presents Retrieval-Augmented Generation (RAG), which fuses a neural generator’s parametric knowledge with non-parametric retrieval over external documents. Two variants are explored: RAG-Token, which can blend information across multiple sources during token generation, and RAG-Sequence. Across tasks like Jeopardy-style question generation and MS MARCO, these models produce more specific and factually accurate outputs than a BART baseline, while also yielding higher diversity (RAG-Sequence > RAG-Token > BART). A case study shows how retrieval guides generation when producing book titles (e.g., “The Sun Also Rises,” “A Farewell to Arms”), after which the generator’s parameters can complete the titles—illustrating cooperation between retrieved evidence and stored knowledge. In fact verification (FEVER), performance is close to stronger pipeline systems: within 4.3% for 3-way classification, and within 2.7% of a RoBERTa model trained with gold evidence for 2-way classification, despite retrieving its own evidence; retrieved top-1 documents match gold articles 71% of the time and top-10 reach 90%. Learned, differentiable retrieval improves results across tasks; a BM25 retriever excels on FEVER, while dense retrieval brings larger gains elsewhere, particularly in open-domain QA. Overall, RAG leverages retrieval to steer generation and surface memorized knowledge, improving factuality, specificity, and diversity without complex task-specific pipelines.\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "d3fedea0-f2a9-46bb-8aaf-287df65b8fff",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:06:04.480181Z",
     "start_time": "2025-09-11T08:06:04.472286Z"
    }
   },
   "source": [
    "print(len(response.source_nodes))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:19:36.261347Z",
     "start_time": "2025-09-11T08:19:02.680711Z"
    }
   },
   "source": [
    "response = query_engine.query(\n",
    "    \"How do agents share information with other agents?\"\n",
    ")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:49:12,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-11 13:49:12,409 - INFO - Selecting query engine 0: The question asks for a specific mechanism/detail from the MetaGPT paper, requiring retrieval of precise context rather than a general summary..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mSelecting query engine 0: The question asks for a specific mechanism/detail from the MetaGPT paper, requiring retrieval of precise context rather than a general summary..\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:49:36,252 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Use a shared external memory: Maintain a common, non-parametric knowledge store (e.g., a dense index of text passages such as Wikipedia).\n",
      "- Retrieve, don’t message: Each agent encodes its query, performs Maximum Inner Product Search over the shared index to retrieve top-K relevant passages, and conditions its output on those passages.\n",
      "- Flexible conditioning: Agents can condition on the same retrieved passages for an entire output or vary passages token-by-token, enabling aggregation of evidence from multiple sources.\n",
      "- Immediate updates: To share new or revised information, update or swap the shared index (add/edit documents). All agents that use this index gain the information immediately, without retraining.\n",
      "- Provenance and interpretability: Because the memory consists of raw, human-readable text, agents’ outputs can be traced back to retrieved evidence.\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "aed060ee",
   "metadata": {},
   "source": [
    "## Let's put everything together"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T08:25:12.797382Z",
     "start_time": "2025-09-11T08:25:12.790901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_router_query_engine(pdf_path):\n",
    "    # Load environment variables\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Load and split document\n",
    "    documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    # Set LLM and embedding model\n",
    "    Settings.llm = OpenAI(model=\"gpt-4.1-mini\", api_key=OPENAI_API_KEY)\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "    # Create indices\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "    # Create query engines\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "    # Create tools (descriptions optional)\n",
    "    summary_tool = QueryEngineTool.from_defaults(query_engine=summary_query_engine)\n",
    "    vector_tool = QueryEngineTool.from_defaults(query_engine=vector_query_engine)\n",
    "\n",
    "    # Create and return router query engine\n",
    "    query_engine = RouterQueryEngine(\n",
    "        selector=LLMSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[summary_tool, vector_tool],\n",
    "        verbose=True\n",
    "    )\n",
    "    return query_engine"
   ],
   "id": "b0086edb9c9cf22d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T08:25:19.565955Z",
     "start_time": "2025-09-11T08:25:16.786059Z"
    }
   },
   "cell_type": "code",
   "source": "query_engine = get_router_query_engine(\"rag.pdf\")",
   "id": "90527b074701a2db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:55:18,815 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "ec1a43f3-77dc-472a-8adc-56551c00a0ff",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-11T08:27:35.219648Z",
     "start_time": "2025-09-11T08:27:24.183513Z"
    }
   },
   "source": [
    "response = query_engine.query(\"what are the important aspects of rag?\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:57:25,853 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-11 13:57:25,858 - INFO - Selecting query engine 0: Both choices provide the same summary, but choice 1 is selected as it is the first instance of the relevant summary describing the important aspect of RAG..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mSelecting query engine 0: Both choices provide the same summary, but choice 1 is selected as it is the first instance of the relevant summary describing the important aspect of RAG..\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:57:35,213 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important aspects of Retrieval-Augmented Generation (RAG) include:\n",
      "\n",
      "1. Hybrid Memory Architecture: RAG combines parametric memory, represented by a pre-trained sequence-to-sequence (seq2seq) model (such as BART), with non-parametric memory, which is a dense vector index of external documents (e.g., Wikipedia) accessed via a neural retriever (such as Dense Passage Retriever, DPR).\n",
      "\n",
      "2. Retrieval-Augmented Generation: The model retrieves relevant documents conditioned on the input query and uses these documents as additional context to generate the output sequence. This approach allows the model to access and incorporate up-to-date and extensive external knowledge beyond what is stored in its parameters.\n",
      "\n",
      "3. Two Model Variants: \n",
      "   - RAG-Sequence: Uses the same retrieved document to generate the entire output sequence, marginalizing over the top-K retrieved documents.\n",
      "   - RAG-Token: Allows different documents to be used for generating each token in the output sequence, enabling the model to combine information from multiple documents dynamically.\n",
      "\n",
      "4. End-to-End Training: Both the retriever and generator components are jointly fine-tuned end-to-end without requiring direct supervision on which documents to retrieve, optimizing the marginal likelihood of the target output.\n",
      "\n",
      "5. Improved Performance on Knowledge-Intensive Tasks: RAG achieves state-of-the-art results on several open-domain question answering datasets and performs well on fact verification, abstractive question answering, and question generation tasks. It outperforms purely parametric models and specialized retrieval-extraction architectures.\n",
      "\n",
      "6. Generation Quality: RAG models generate more factual, specific, and diverse language compared to parametric-only baselines, reducing hallucinations and improving factual accuracy.\n",
      "\n",
      "7. Flexibility and Updatability: The non-parametric memory (document index) can be replaced or updated without retraining the entire model, allowing the system to adapt to changes in world knowledge efficiently.\n",
      "\n",
      "8. Interpretability: Since the model retrieves and conditions on explicit text passages, it provides a form of provenance and interpretability for its generated outputs.\n",
      "\n",
      "9. Efficient Decoding Strategies: RAG employs decoding methods such as \"Thorough Decoding\" and \"Fast Decoding\" to handle the marginalization over retrieved documents during generation.\n",
      "\n",
      "10. Scalability: The retrieval component uses Maximum Inner Product Search (MIPS) with efficient approximate nearest neighbor search algorithms, enabling fast retrieval from millions of documents.\n",
      "\n",
      "These aspects collectively enable RAG to effectively integrate retrieval and generation, making it a powerful and flexible architecture for knowledge-intensive natural language processing tasks.\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
